{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from utils.polyhierarchy_comparison_measures import SP_weighted_embeddings_wordnet,SP_max_embeddings_wordnet\n",
    "import itertools\n",
    "import logging\n",
    "import operator\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import networkx as nx\n",
    "import glob\n",
    "from scipy import stats\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "wn_lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldDatapath= './datasets/'\n",
    "emb_data = glob.glob(\"./Embeddings/*.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Embeddings/res>VERB_yp-130.csv\n",
      "./Embeddings/res>NOUN_rg-65.csv\n",
      "./Embeddings/res>VERB_verb-143.csv\n",
      "./Embeddings/res>NOUN_mc-30.csv\n"
     ]
    }
   ],
   "source": [
    "for file in emb_data:\n",
    "    print (file)\n",
    "    algoname = file.split('/')[2].split('sss')[0]\n",
    "    \n",
    "    filename = file.split('/')[-1].split('>')[1]\n",
    "    # read human similarity score\n",
    "    df = pd.read_csv(goldDatapath+filename)\n",
    "    score_human = df['similarity'].values\n",
    "    norm1 = [(float(i)-min(score_human))/(max(score_human )-min(score_human )) for i in score_human ] # normalize score in 0-1\n",
    "    \n",
    "    df_r = pd.read_csv(file)\n",
    "    \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldDatapath= './datasets/'\n",
    "emb_data = glob.glob(\"./Embeddings/*.csv\")\n",
    "\n",
    "for file in emb_data:\n",
    "    print (file)\n",
    "    algoname = file.split('/')[2].split('sss')[0]\n",
    "    \n",
    "    filename = file.split('/')[-1].split('>')[1]\n",
    "    # read human similarity score\n",
    "    df = pd.read_csv(goldDatapath+filename)\n",
    "    score_human = df['similarity'].values\n",
    "    norm1 = [(float(i)-min(score_human))/(max(score_human )-min(score_human )) for i in score_human ] # normalize score in 0-1\n",
    "    \n",
    "    for algoname in algos:\n",
    "        # read similarity file\n",
    "        df_r = pd.read_csv(file)\n",
    "        \n",
    "        algo_score = df_r['sim'].values\n",
    "        # normalize score\n",
    "        if (df_r['sim'] > 1).any():\n",
    "            algo_score= [(float(i)-min(algo_score))/(max(algo_score )-min(algo_score )) for i in algo_score ] # normalize score in 0-1\n",
    "    \n",
    "        # calculate Pearson's correlation\n",
    "        \n",
    "        corrp, _ = pearsonr(norm1, algo_score)\n",
    "        pearson_ls.append(corrp) \n",
    "        print (corrp)\n",
    "        corrs, _ = spearmanr(norm1, algo_score)\n",
    "        #print('Spearmans correlation: %.3f' % corr)\n",
    "        spearman_ls.append(corrs) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
